{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7fa17d",
   "metadata": {},
   "source": [
    "Building a simple cnn Arcchitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10d8f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,605,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,605,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,626,442</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,626,442\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,626,442</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,626,442\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# --- Define Model Parameters ---\n",
    "# Let's assume our images will be resized to 64x64 pixels with 3 color channels (RGB)\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "# Let's assume we have 10 fashion categories to predict\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# --- Build the CNN Model ---\n",
    "model = keras.Sequential([\n",
    "    # Input Layer: Specify the shape of our images\n",
    "    layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    \n",
    "    # First Convolutional Block \n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten the 2D feature maps into a 1D vector\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # A standard Dense layer for classification\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    \n",
    "    # Output Layer: NUM_CLASSES neurons with softmax for multi-class probability\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print the model's summary to see how the data shape changes\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb8634",
   "metadata": {},
   "source": [
    "Formular for Convolutional layer = (input - kernel size + 1)\n",
    "Formular for Max pooling layer = floor(input/pool size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae75ba8",
   "metadata": {},
   "source": [
    "#### CNN Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f910e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garment Lower body: 10 images copied to ../data/cnn_data/Garment Lower body\n",
      "Socks & Tights: 10 images copied to ../data/cnn_data/Socks & Tights\n",
      "Underwear: 10 images copied to ../data/cnn_data/Underwear\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "CLASSES = [\"Garment Lower body\", \"Socks & Tights\", \"Underwear\"]  # Classes to include\n",
    "N_PER_CLASS = 10  # Number of images per class\n",
    "ARTICLES_CSV = \"../data/articles.csv\"  # Path to your CSV file\n",
    "ZIP_PATH = \"../data/images.zip\"  # Path to downloaded zip file\n",
    "OUTPUT_DIR = \"../data/cnn_data/\"  # Directory where extracted images will be saved\n",
    "# ----------------------------------------\n",
    "\n",
    "# Create output directories for each class\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)  # Create root output folder if it doesn't exist\n",
    "for c in CLASSES:\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, c), exist_ok=True)  # Create subfolder per class\n",
    "\n",
    "# Load CSV and filter for chosen classes\n",
    "df = pd.read_csv(ARTICLES_CSV)  # Read articles CSV\n",
    "df['product_group_name'] = df['product_group_name'].str.strip()  # Remove extra spaces\n",
    "filtered = df[df['product_group_name'].isin(CLASSES)]  # Keep only rows for selected classes\n",
    "\n",
    "# Select top N_PER_CLASS articles per class\n",
    "selected = pd.DataFrame()  # Empty DataFrame to store selected articles\n",
    "for c in CLASSES:\n",
    "    subset = filtered[filtered['product_group_name'] == c].head(N_PER_CLASS)  # Take first N_PER_CLASS\n",
    "    selected = pd.concat([selected, subset])  # Add to selection\n",
    "selected = selected.reset_index(drop=True)  # Reset row indices\n",
    "\n",
    "# Zero-pad article_ids to 10 digits to match zip filenames\n",
    "selected['article_id'] = selected['article_id'].astype(str).str.zfill(10)\n",
    "\n",
    "# Map article_id to its class\n",
    "article_class_map = dict(zip(selected['article_id'], selected['product_group_name']))  # Dict: id -> class\n",
    "\n",
    "# Set of article_ids we need\n",
    "needed_ids = set(selected['article_id'])  # For quick lookup\n",
    "\n",
    "# ----------------- Extraction -----------------\n",
    "with zipfile.ZipFile(ZIP_PATH, 'r') as z:  # Open the zip file\n",
    "    for member in z.namelist():  # Loop over all files inside the zip\n",
    "        if not member.lower().endswith('.jpg'):\n",
    "            continue  # Skip non-image files\n",
    "        article_id = os.path.splitext(os.path.basename(member))[0]  # Get article ID from filename\n",
    "        if article_id in needed_ids:  # Only extract if we need it\n",
    "            cls = article_class_map[article_id]  # Get the class of this article\n",
    "            dst = os.path.join(OUTPUT_DIR, cls, f\"{article_id}.jpg\")  # Destination path\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)  # Ensure folder exists\n",
    "            with z.open(member) as source, open(dst, \"wb\") as target:  # Open source & destination\n",
    "                shutil.copyfileobj(source, target)  # Copy image content\n",
    "\n",
    "# ----------------- Report -----------------\n",
    "for c in CLASSES:\n",
    "    cnt = len([f for f in os.listdir(os.path.join(OUTPUT_DIR, c)) if f.lower().endswith('.jpg')])\n",
    "    print(f\"{c}: {cnt} images copied to {os.path.join(OUTPUT_DIR, c)}\")  # Print how many images copied per class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f6664",
   "metadata": {},
   "source": [
    "#### Preparing dataset for training,validation and also parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d14106df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 files belonging to 3 classes.\n",
      "Using 24 files for training.\n",
      "Found 30 files belonging to 3 classes.\n",
      "Using 6 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # Import TensorFlow library\n",
    "\n",
    "# ---------------- Image & dataset parameters ----------------\n",
    "IMG_HEIGHT = 64  # Height to resize all images to\n",
    "IMG_WIDTH = 64   # Width to resize all images to\n",
    "BATCH_SIZE = 32  # Number of images per batch during training\n",
    "DATA_DIR = '../data/cnn_data/'  # Root folder where your class subfolders are\n",
    "\n",
    "# ---------------- Create the training dataset ----------------\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,               # Directory containing subfolders per class\n",
    "    validation_split=0.2,   # Reserve 20% of data for validation\n",
    "    subset=\"training\",      # Specify this dataset is the training portion\n",
    "    seed=123,               # Random seed for reproducibility (ensures same split every run)\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),  # Resize all images to uniform size\n",
    "    batch_size=BATCH_SIZE   # Number of images per batch\n",
    ")\n",
    "\n",
    "# ---------------- Create the validation dataset ----------------\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,               # Same directory as training\n",
    "    validation_split=0.2,   # Same split percentage\n",
    "    subset=\"validation\",    # Specify this dataset is the validation portion\n",
    "    seed=123,               # Same seed to ensure split matches training\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),  # Resize images to same size as training\n",
    "    batch_size=BATCH_SIZE   # Same batch size as training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98196093",
   "metadata": {},
   "source": [
    "#### Preparation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b909c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',  # Specifies the optimization algorithm used to update model weights (Adam is fast and commonly used)\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  \n",
    "        # Defines the loss function to measure how well the model predicts labels\n",
    "        # 'SparseCategoricalCrossentropy' is used when labels are integers (not one-hot encoded)\n",
    "        # 'from_logits=True' means the model's output layer does NOT have a softmax; loss will apply softmax internally\n",
    "    metrics=['accuracy']  # Metric to track during training; here we monitor accuracy of predictions\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting CNN training...\")  \n",
    "# Displays a message indicating that model training is about to begin\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,               # The training dataset containing input images and labels\n",
    "    validation_data=val_ds, # The validation dataset used to evaluate performance after each epoch\n",
    "    epochs=5                # Number of times the model will go through the entire training dataset\n",
    ")\n",
    "# 'model.fit' trains the CNN on the training data and tracks performance on validation data\n",
    "# Returns a 'history' object containing training and validation loss and accuracy per epoch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
