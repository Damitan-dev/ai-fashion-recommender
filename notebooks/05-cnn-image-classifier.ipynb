{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7fa17d",
   "metadata": {},
   "source": [
    "Building a simple cnn Arcchitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10d8f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,605,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,605,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,626,442</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,626,442\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,626,442</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,626,442\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# --- Define Model Parameters ---\n",
    "# Let's assume our images will be resized to 64x64 pixels with 3 color channels (RGB)\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "# Let's assume we have 10 fashion categories to predict\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# --- Build the CNN Model ---\n",
    "model = keras.Sequential([\n",
    "    # Input Layer: Specify the shape of our images\n",
    "    layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    \n",
    "    # First Convolutional Block \n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten the 2D feature maps into a 1D vector\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # A standard Dense layer for classification\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    \n",
    "    # Output Layer: NUM_CLASSES neurons with softmax for multi-class probability\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print the model's summary to see how the data shape changes\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb8634",
   "metadata": {},
   "source": [
    "Formular for Convolutional layer = (input - kernel size + 1)\n",
    "Formular for Max pooling layer = floor(input/pool size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae75ba8",
   "metadata": {},
   "source": [
    "#### CNN Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f910e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garment Lower body: 10 images copied to ../data/cnn_data/Garment Lower body\n",
      "Socks & Tights: 10 images copied to ../data/cnn_data/Socks & Tights\n",
      "Underwear: 10 images copied to ../data/cnn_data/Underwear\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "CLASSES = [\"Garment Lower body\", \"Socks & Tights\", \"Underwear\"]  # Classes to include\n",
    "N_PER_CLASS = 10  # Number of images per class\n",
    "ARTICLES_CSV = \"../data/articles.csv\"  # Path to your CSV file\n",
    "ZIP_PATH = \"../data/images.zip\"  # Path to downloaded zip file\n",
    "OUTPUT_DIR = \"../data/cnn_data/\"  # Directory where extracted images will be saved\n",
    "# ----------------------------------------\n",
    "\n",
    "# Create output directories for each class\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)  # Create root output folder if it doesn't exist\n",
    "for c in CLASSES:\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, c), exist_ok=True)  # Create subfolder per class\n",
    "\n",
    "# Load CSV and filter for chosen classes\n",
    "df = pd.read_csv(ARTICLES_CSV)  # Read articles CSV\n",
    "df['product_group_name'] = df['product_group_name'].str.strip()  # Remove extra spaces\n",
    "filtered = df[df['product_group_name'].isin(CLASSES)]  # Keep only rows for selected classes\n",
    "\n",
    "# Select top N_PER_CLASS articles per class\n",
    "selected = pd.DataFrame()  # Empty DataFrame to store selected articles\n",
    "for c in CLASSES:\n",
    "    subset = filtered[filtered['product_group_name'] == c].head(N_PER_CLASS)  # Take first N_PER_CLASS\n",
    "    selected = pd.concat([selected, subset])  # Add to selection\n",
    "selected = selected.reset_index(drop=True)  # Reset row indices\n",
    "\n",
    "# Zero-pad article_ids to 10 digits to match zip filenames\n",
    "selected['article_id'] = selected['article_id'].astype(str).str.zfill(10)\n",
    "\n",
    "# Map article_id to its class\n",
    "article_class_map = dict(zip(selected['article_id'], selected['product_group_name']))  # Dict: id -> class\n",
    "\n",
    "# Set of article_ids we need\n",
    "needed_ids = set(selected['article_id'])  # For quick lookup\n",
    "\n",
    "# ----------------- Extraction -----------------\n",
    "with zipfile.ZipFile(ZIP_PATH, 'r') as z:  # Open the zip file\n",
    "    for member in z.namelist():  # Loop over all files inside the zip\n",
    "        if not member.lower().endswith('.jpg'):\n",
    "            continue  # Skip non-image files\n",
    "        article_id = os.path.splitext(os.path.basename(member))[0]  # Get article ID from filename\n",
    "        if article_id in needed_ids:  # Only extract if we need it\n",
    "            cls = article_class_map[article_id]  # Get the class of this article\n",
    "            dst = os.path.join(OUTPUT_DIR, cls, f\"{article_id}.jpg\")  # Destination path\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)  # Ensure folder exists\n",
    "            with z.open(member) as source, open(dst, \"wb\") as target:  # Open source & destination\n",
    "                shutil.copyfileobj(source, target)  # Copy image content\n",
    "\n",
    "# ----------------- Report -----------------\n",
    "for c in CLASSES:\n",
    "    cnt = len([f for f in os.listdir(os.path.join(OUTPUT_DIR, c)) if f.lower().endswith('.jpg')])\n",
    "    print(f\"{c}: {cnt} images copied to {os.path.join(OUTPUT_DIR, c)}\")  # Print how many images copied per class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f6664",
   "metadata": {},
   "source": [
    "#### Preparing dataset for training,validation and also parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d14106df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 files belonging to 3 classes.\n",
      "Using 24 files for training.\n",
      "Found 30 files belonging to 3 classes.\n",
      "Using 6 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # Import TensorFlow library\n",
    "\n",
    "# ---------------- Image & dataset parameters ----------------\n",
    "IMG_HEIGHT = 64  # Height to resize all images to\n",
    "IMG_WIDTH = 64   # Width to resize all images to\n",
    "BATCH_SIZE = 32  # Number of images per batch during training\n",
    "DATA_DIR = '../data/cnn_data/'  # Root folder where your class subfolders are\n",
    "\n",
    "# ---------------- Create the training dataset ----------------\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,               # Directory containing subfolders per class\n",
    "    validation_split=0.2,   # Reserve 20% of data for validation\n",
    "    subset=\"training\",      # Specify this dataset is the training portion\n",
    "    seed=123,               # Random seed for reproducibility (ensures same split every run)\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),  # Resize all images to uniform size\n",
    "    batch_size=BATCH_SIZE   # Number of images per batch\n",
    ")\n",
    "\n",
    "# ---------------- Create the validation dataset ----------------\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,               # Same directory as training\n",
    "    validation_split=0.2,   # Same split percentage\n",
    "    subset=\"validation\",    # Specify this dataset is the validation portion\n",
    "    seed=123,               # Same seed to ensure split matches training\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),  # Resize images to same size as training\n",
    "    batch_size=BATCH_SIZE   # Same batch size as training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98196093",
   "metadata": {},
   "source": [
    "#### Preparation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b909c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CNN training...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Projects\\ai-fashion-recommender\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:717: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.0000e+00 - loss: 67.4985 - val_accuracy: 0.5000 - val_loss: 11.6899\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.3333 - loss: 16.7364 - val_accuracy: 0.3333 - val_loss: 331.4742\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.4583 - loss: 204.2340 - val_accuracy: 0.3333 - val_loss: 475.2529\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.3333 - loss: 376.1746 - val_accuracy: 0.3333 - val_loss: 311.9909\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.3333 - loss: 212.8883 - val_accuracy: 0.3333 - val_loss: 191.9614\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',  # Specifies the optimization algorithm used to update model weights (Adam is fast and commonly used)\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  \n",
    "        # Defines the loss function to measure how well the model predicts labels\n",
    "        # 'SparseCategoricalCrossentropy' is used when labels are integers (not one-hot encoded)\n",
    "        # 'from_logits=True' means the model's output layer does NOT have a softmax; loss will apply softmax internally\n",
    "    metrics=['accuracy']  # Metric to track during training; here we monitor accuracy of predictions\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting CNN training...\")  \n",
    "# Displays a message indicating that model training is about to begin\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,               # The training dataset containing input images and labels\n",
    "    validation_data=val_ds, # The validation dataset used to evaluate performance after each epoch\n",
    "    epochs=5                # Number of times the model will go through the entire training dataset\n",
    ")\n",
    "# 'model.fit' trains the CNN on the training data and tracks performance on validation data\n",
    "# Returns a 'history' object containing training and validation loss and accuracy per epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea5441",
   "metadata": {},
   "source": [
    "#### Boosting Performance  with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a707d8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_20048\\476013090.py:9: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = tf.keras.applications.MobileNetV2(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │         \u001b[38;5;34m3,843\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,261,827</span> (8.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,261,827\u001b[0m (8.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span> (15.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,843\u001b[0m (15.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15s/step - accuracy: 0.4167 - loss: 1.7899 - val_accuracy: 0.6667 - val_loss: 1.0742\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - accuracy: 0.4167 - loss: 1.2979 - val_accuracy: 0.8333 - val_loss: 0.7939\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.5000 - loss: 0.9930 - val_accuracy: 0.8333 - val_loss: 0.7043\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.5833 - loss: 0.8491 - val_accuracy: 0.6667 - val_loss: 0.7082\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.5833 - loss: 0.7954 - val_accuracy: 0.5000 - val_loss: 0.7507\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.6250 - loss: 0.7793 - val_accuracy: 0.5000 - val_loss: 0.7983\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.6250 - loss: 0.7711 - val_accuracy: 0.5000 - val_loss: 0.8300\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 0.6250 - loss: 0.7557 - val_accuracy: 0.5000 - val_loss: 0.8360\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.6250 - loss: 0.7273 - val_accuracy: 0.5000 - val_loss: 0.8168\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.6250 - loss: 0.6864 - val_accuracy: 0.6667 - val_loss: 0.7790\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # Import TensorFlow for deep learning\n",
    "\n",
    "# Reuse constants from before\n",
    "IMG_HEIGHT = 64  # Height of input images (must match pre-trained model's expected size)\n",
    "IMG_WIDTH = 64   # Width of input images\n",
    "NUM_CLASSES = 3  # Number of target categories (e.g., Dresses, Upper body, Underwear)\n",
    "\n",
    "# -------------------- Task 1: Load Pre-trained Base Model --------------------\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),  # Shape of input images (H, W, 3 for RGB)\n",
    "    include_top=False,                       # Remove the original ImageNet classification head\n",
    "    weights='imagenet'                        # Load weights pre-trained on ImageNet dataset\n",
    ")  # This gives us a powerful feature extractor without retraining from scratch\n",
    "\n",
    "# -------------------- Task 2: Freeze the Base Model --------------------\n",
    "base_model.trainable = False  # Keep pre-trained weights fixed so they don’t get updated during training\n",
    "\n",
    "# -------------------- Task 3: Build the New Model --------------------\n",
    "model_transfer = tf.keras.Sequential([\n",
    "    base_model,                                # Use frozen pre-trained CNN as the feature extractor\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),  # Convert feature maps to a single vector by averaging each feature map\n",
    "    tf.keras.layers.Dense(NUM_CLASSES)         # Final dense layer to classify into our custom categories\n",
    "])  # This stack lets us reuse old knowledge and just learn the mapping to our new labels\n",
    "\n",
    "# Show model details\n",
    "model_transfer.summary()  # Displays number of trainable vs non-trainable parameters\n",
    "\n",
    "# -------------------- Task 4: Compile the Model --------------------\n",
    "model_transfer.compile(\n",
    "    optimizer='adam',  # Adam optimizer adapts learning rate for faster convergence\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # Loss for integer class labels\n",
    "    metrics=['accuracy']  # Track accuracy during training\n",
    ")\n",
    "\n",
    "# -------------------- Task 5: Train the Model --------------------\n",
    "epochs = 10  # More epochs since training will be faster with frozen layers\n",
    "history_transfer = model_transfer.fit(\n",
    "    train_ds,              # Training dataset\n",
    "    epochs=epochs,         # Number of passes through the dataset\n",
    "    validation_data=val_ds # Validation dataset to monitor generalization\n",
    ")  # We expect faster accuracy improvement compared to training a CNN from scratch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
