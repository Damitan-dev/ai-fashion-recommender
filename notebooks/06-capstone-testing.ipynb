{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03a8910",
   "metadata": {},
   "source": [
    "####    To test run the recommender python code(Hybrid recommender which consists of the three models) to check if it works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28cd560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the Hybrid Recommender...\n",
      "Loading artifacts from c:\\Users\\hp\\Projects\\ai-fashion-recommender\\artifacts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All artifacts loaded successfully.\n",
      "\\n--- Final Hybrid Recommendations ---\n",
      "[np.int64(322017038), np.int64(557276005), np.int64(573086001), np.int64(551892005), np.int64(579833002), np.int64(602138001), np.int64(605106007), np.int64(592180005), np.int64(572280001), np.int64(551892004)]\n",
      "[322017038, 557276005, 573086001, 551892005, 579833002, 602138001, 605106007, 592180005, 572280001, 551892004]\n",
      "['322017038', '557276005', '573086001', '551892005', '579833002', '602138001', '605106007', '592180005', '572280001', '551892004']\n"
     ]
    }
   ],
   "source": [
    "# Add project root to path to allow imports from src\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.recommender import HybridRecommender\n",
    "\n",
    "# This should run without any errors if your loading logic is correct\n",
    "recommender_engine = HybridRecommender(artifacts_path=\"../artifacts\")\n",
    "\n",
    "\n",
    "  # Prints the full first article_id\n",
    "\n",
    "\n",
    "\n",
    "test_user ='000058a12d5b43e67d225668fa1f8d618c13dc232df0cad8ffe7ad4a1091e318'\n",
    "test_article = '372860002'\n",
    "\n",
    "final_recommendations = recommender_engine.get_recommendations(test_user, test_article)\n",
    "\n",
    "print(\"\\\\n--- Final Hybrid Recommendations ---\")\n",
    "print(final_recommendations)\n",
    "\n",
    "# Convert to normal Python ints\n",
    "final_recs_clean = [int(a) for a in final_recommendations]\n",
    "print(final_recs_clean)\n",
    "\n",
    "# Or convert to strings\n",
    "final_recs_str = [str(a) for a in final_recommendations]\n",
    "print(final_recs_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41210975",
   "metadata": {},
   "source": [
    "#### Code to Check the files in the artifacts folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bf7b8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved artifacts path: c:\\Users\\hp\\Projects\\ai-fashion-recommender\\artifacts\n",
      "Files in artifacts folder: ['article_encoder.pkl', 'cnn_extractor.h5', 'collab_model.h5', 'cosine_sim_matrix.pkl', 'image_embeddings.npy', 'image_paths.pkl', 'tfidf_matrix.pkl', 'tfidf_vectorizer.pkl', 'user_encoder.pkl', 'user_purchases.pkl', 'visual_sim_matrix.pkl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "artifacts_path = os.path.abspath(\"../artifacts\")\n",
    "print(\"Resolved artifacts path:\", artifacts_path)\n",
    "print(\"Files in artifacts folder:\", os.listdir(artifacts_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e284bce",
   "metadata": {},
   "source": [
    "#### Code to check if the files you want to check exists in the artifacts folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5f2f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_encoder.pkl exists: True\n",
      "article_encoder.pkl exists: True\n",
      "tfidf_vectorizer.pkl exists: True\n",
      "tfidf_matrix.pkl exists: True\n",
      "cosine_sim_matrix.pkl exists: True\n",
      "cnn_extractor.h5 exists: True\n",
      "image_embeddings.npy exists: True\n",
      "image_paths.pkl exists: True\n",
      "visual_sim_matrix.pkl exists: True\n",
      "user_purchases.pkl exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "artifacts_path = '../artifacts/'\n",
    "\n",
    "files = [\n",
    "    'user_encoder.pkl',\n",
    "    'article_encoder.pkl',\n",
    "    'tfidf_vectorizer.pkl',\n",
    "    'tfidf_matrix.pkl',\n",
    "    'cosine_sim_matrix.pkl',\n",
    "    'cnn_extractor.h5',\n",
    "    'image_embeddings.npy',\n",
    "    'image_paths.pkl',\n",
    "    'visual_sim_matrix.pkl',\n",
    "    'user_purchases.pkl'\n",
    "]\n",
    "\n",
    "for f in files:\n",
    "    path = os.path.join(artifacts_path, f)\n",
    "    print(f\"{f} exists: {os.path.exists(path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b1dcd",
   "metadata": {},
   "source": [
    "#### Debugging code to check what the files will look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ee90614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User encoder sample: ['0000423b00ade91418cceaf3b26c6af3dd342b51fd051eec9c12fb36984420fa', '000058a12d5b43e67d225668fa1f8d618c13dc232df0cad8ffe7ad4a1091e318', '00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2', '0000f1c71aafe5963c3d195cf273f7bfd50bbf17761c9199e53dbb81641becd7', '00015c1a121e08bbd2552c15fbbb6e6b19d3bf8f7b6a3d60c6d7be26f06264d6']\n",
      "Article encoder sample: [np.int64(108775015), np.int64(108775044), np.int64(108775051), np.int64(110065001), np.int64(110065002)]\n",
      "TF-IDF matrix shape: (5000, 1381)\n",
      "Cosine sim matrix shape: (5000, 5000)\n",
      "Image embeddings shape: (1188, 1280)\n",
      "Visual sim matrix shape: (1188, 1188)\n",
      "Sample user purchases (1): [21488, 5005]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load encoders\n",
    "user_encoder = joblib.load(os.path.join(artifacts_path, 'user_encoder.pkl'))\n",
    "article_encoder = joblib.load(os.path.join(artifacts_path, 'article_encoder.pkl'))\n",
    "\n",
    "print(\"User encoder sample:\", list(user_encoder.classes_)[:5])\n",
    "print(\"Article encoder sample:\", list(article_encoder.classes_)[:5])\n",
    "\n",
    "# Load matrices\n",
    "tfidf_matrix = joblib.load(os.path.join(artifacts_path, 'tfidf_matrix.pkl'))\n",
    "cosine_sim_matrix = joblib.load(os.path.join(artifacts_path, 'cosine_sim_matrix.pkl'))\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "print(\"Cosine sim matrix shape:\", cosine_sim_matrix.shape)\n",
    "\n",
    "# Load visual embeddings\n",
    "image_embeddings = np.load(os.path.join(artifacts_path, 'image_embeddings.npy'))\n",
    "print(\"Image embeddings shape:\", image_embeddings.shape)\n",
    "\n",
    "# Load visual similarity\n",
    "visual_sim_matrix = joblib.load(os.path.join(artifacts_path, 'visual_sim_matrix.pkl'))\n",
    "print(\"Visual sim matrix shape:\", visual_sim_matrix.shape)\n",
    "\n",
    "# Load user purchases\n",
    "user_purchases = joblib.load(os.path.join(artifacts_path, 'user_purchases.pkl'))\n",
    "some_user = list(user_purchases.keys())[1]\n",
    "print(f\"Sample user purchases ({some_user}):\", list(user_purchases[some_user])[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a683b1",
   "metadata": {},
   "source": [
    "#### list to check article_ids to be used for testing the streamlit recommendation app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e0b9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 valid article IDs:\n",
      "[108775015 108775044 108775051 110065001 110065002 110065011 111565001\n",
      " 111565003 111586001 111593001]\n",
      "\n",
      "Random valid article IDs:\n",
      "[774785001 829643005 811235001 573085043 485689020 783707046 674695001\n",
      " 538704001 717132001 888157002]\n"
     ]
    }
   ],
   "source": [
    "# Load your saved article encoder\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load the encoder\n",
    "article_encoder = joblib.load(\"../artifacts/article_encoder.pkl\")\n",
    "\n",
    "# Show the first 10 valid article IDs\n",
    "print(\"First 10 valid article IDs:\")\n",
    "print(article_encoder.classes_[:10])\n",
    "\n",
    "# Or get 10 random valid IDs\n",
    "random_ids = np.random.choice(article_encoder.classes_, size=10, replace=False)\n",
    "print(\"\\nRandom valid article IDs:\")\n",
    "print(random_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e652c2a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
